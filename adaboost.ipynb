{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4b87aae",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-25T15:18:01.593852Z",
     "iopub.status.busy": "2025-10-25T15:18:01.593528Z",
     "iopub.status.idle": "2025-10-25T15:18:03.412381Z",
     "shell.execute_reply": "2025-10-25T15:18:03.411359Z"
    },
    "papermill": {
     "duration": 1.823459,
     "end_time": "2025-10-25T15:18:03.413830",
     "exception": false,
     "start_time": "2025-10-25T15:18:01.590371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/sample_submission.csv\n",
      "/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv\n",
      "/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e858e7b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-25T15:18:03.419221Z",
     "iopub.status.busy": "2025-10-25T15:18:03.418845Z",
     "iopub.status.idle": "2025-10-25T15:18:09.460035Z",
     "shell.execute_reply": "2025-10-25T15:18:09.459081Z"
    },
    "papermill": {
     "duration": 6.045455,
     "end_time": "2025-10-25T15:18:09.461427",
     "exception": false,
     "start_time": "2025-10-25T15:18:03.415972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete.\n",
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "#Importing libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import AdaBoostRegressor  \n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "# Configuration and Constants \n",
    "CLIP_FLOOR = 1.0     #Negative/zero costs clipped to this value before log transform\n",
    "FREQ_THRESHOLD = 0.05 #Group categories with frequency < 5%\n",
    "N_SPLITS_CV = 10         \n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Data Loading\n",
    "try:\n",
    "    train_df = pd.read_csv(\"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/train.csv\")\n",
    "    test_df = pd.read_csv(\"/kaggle/input/Medical-Equipments-Cost-Prediction-Challenge/test.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Data files not found.\")\n",
    "    exit()\n",
    "\n",
    "test_ids = test_df['Hospital_Id']\n",
    "train_df.set_index('Hospital_Id', inplace=True) \n",
    "test_df.set_index('Hospital_Id', inplace=True)\n",
    "\n",
    "# Feature Engineering\n",
    "y_train = train_df[\"Transport_Cost\"].copy()\n",
    "X_train = train_df.drop(columns=[\"Transport_Cost\"]).copy()\n",
    "X_test = test_df.copy()\n",
    "\n",
    "combined_df = pd.concat([X_train, X_test], axis=0)\n",
    "\n",
    "# Best-performing transformation: Clip and Log\n",
    "y_train_transformed = np.log(y_train.clip(lower=CLIP_FLOOR))\n",
    "\n",
    "# KEEPING THE DATE FEATURES\n",
    "combined_df['Order_Placed_Date'] = pd.to_datetime(combined_df['Order_Placed_Date'], format='%m/%d/%y', errors='coerce')\n",
    "combined_df['Delivery_Date'] = pd.to_datetime(combined_df['Delivery_Date'], format='%m/%d/%y', errors='coerce')\n",
    "combined_df['Delivery_Lag_Days'] = (combined_df['Delivery_Date'] - combined_df['Order_Placed_Date']).dt.days.fillna(0).astype(int)\n",
    "combined_df['Order_Day_of_Week'] = combined_df['Order_Placed_Date'].dt.dayofweek\n",
    "combined_df['Order_Month'] = combined_df['Order_Placed_Date'].dt.month\n",
    "\n",
    "combined_df['Equipment_Volume'] = combined_df['Equipment_Height'] * combined_df['Equipment_Width']\n",
    "combined_df['Equipment_Density'] = combined_df['Equipment_Weight'] / (combined_df['Equipment_Volume'] + 1e-6)\n",
    "\n",
    "combined_df.drop(columns=['Order_Placed_Date', 'Delivery_Date', 'Supplier_Name', 'Hospital_Location'], inplace=True, errors='ignore')\n",
    "\n",
    "# Binary and Categorical Mapping\n",
    "binary_map = {'Yes': 1, 'No': 0}\n",
    "binary_cols = ['CrossBorder_Shipping', 'Installation_Service', 'Rural_Hospital', 'Urgent_Shipping', 'Fragile_Equipment']\n",
    "\n",
    "for col in binary_cols:\n",
    "    if col in combined_df.columns:\n",
    "        combined_df[col] = combined_df[col].map(binary_map).fillna(0) # Fill NaNs with 0 ('No')\n",
    "\n",
    "# Group Low-Frequency Categorical Features\n",
    "categorical_cols_to_group = ['Equipment_Type', 'Transport_Method', 'Hospital_Info']\n",
    "for col in categorical_cols_to_group:\n",
    "    if col in combined_df.columns:\n",
    "        train_counts = combined_df.iloc[:len(X_train)][col].value_counts(normalize=True)\n",
    "        low_freq_cats = train_counts[train_counts < FREQ_THRESHOLD].index\n",
    "        combined_df[col] = np.where(combined_df[col].isin(low_freq_cats), 'Other', combined_df[col])\n",
    "\n",
    "# Final Data Preparation\n",
    "X_train_clean = combined_df.iloc[:len(X_train)]\n",
    "X_test_clean = combined_df.iloc[len(X_train):]\n",
    "\n",
    "\n",
    "# Define Preprocessing Pipeline\n",
    "numeric_cols = [col for col in X_train_clean.select_dtypes(include=np.number).columns.tolist() if col not in binary_cols]\n",
    "categorical_cols = X_train_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "#Numerical columns median imputation is done \n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())  \n",
    "])\n",
    "\n",
    "#Categorical columns One-hot encoding is done\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ],\n",
    "    remainder='passthrough' # Passes through the binary columns\n",
    ")\n",
    "          \n",
    "# Fit AdaBoost Model\n",
    "ada_model = AdaBoostRegressor(\n",
    "    n_estimators=300,        # Number of boosting stages\n",
    "    learning_rate=0.1,       # Shrinks contribution of each estimator\n",
    "    loss='square',           # The loss function to use when updating the weights\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Create the full pipeline\n",
    "adaboost_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', ada_model)\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the entire training dataset\n",
    "adaboost_pipeline.fit(X_train_clean, y_train_transformed)\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Predict on test data using the fitted pipeline\n",
    "test_pred_transformed = adaboost_pipeline.predict(X_test_clean)\n",
    "\n",
    "# Reverse log transform\n",
    "test_pred = np.exp(test_pred_transformed).clip(min=1.0) # Clip at 1.0\n",
    "\n",
    "# Create submission file\n",
    "submission = pd.DataFrame({\n",
    "    'Hospital_Id': test_ids,\n",
    "    'Transport_Cost': test_pred\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission file created: submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 14140597,
     "sourceId": 116098,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.01788,
   "end_time": "2025-10-25T15:18:10.082067",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-25T15:17:57.064187",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
